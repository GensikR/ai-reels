🧱 Pipeline Architecture: AI-Generated Viral Shorts
💡 Summary

Generate viral short videos featuring nonhuman characters (monkey, yeti, alien, etc.) in a podcast-style format — using AI to create script, voice, animation, and video editing, producing engaging content optimized for social platforms.
🛠️ Tech Stack

    Backend: FastAPI (Python) — asynchronous, performant API framework

    AI Text Generation: OpenAI GPT-4 / GPT-3.5 API

    Text-to-Speech: ElevenLabs API, Bark by Suno, Play.ht (choose based on cost and quality)

    Talking Head Video: D-ID API, HeyGen, or SadTalker for animated characters

    Video Editing: FFmpeg scripting, CapCut, Runway, Canva (for overlays, captions, final polish)

    Frontend: React (planned) — user input forms and video display

    Storage & Deployment: Cloud storage (AWS S3, GCP Storage), Docker containers, cloud VM or serverless functions

    Misc: Whisper AI for transcription, Pixabay/YouTube Audio Library for sound assets, Buffer/Metricool for social scheduling

🔧 STAGE 1: Input / Topic Seed

Manual or Form Input

    Enter topic idea (e.g., “Can a gorilla beat 100 humans?”)

    Optional: User selects voice style and character (gorilla, alien, yeti)

🤖 STAGE 2: Script Generation (Text)

Tool: GPT-4 or GPT-3.5 via OpenAI API

    Generate a short monologue (~10–20 seconds)

    Add slang, humor, or viral tone

    Example prompt template:

        Write a short, funny podcast-style monologue (15–20 seconds) from the point of view of a gorilla debating if it could defeat 100 unarmed humans. Make it sound confident, a little cocky, and slightly unhinged.

Output: Plain text script
🎤 STAGE 3: Voice Generation (Text-to-Speech)

Tool Options:

    ElevenLabs API (Free tier: 10k characters/month)

    Bark by Suno (Open-source, local or Colab)

    Play.ht or TTSMonster (Web UI or API)

Input: Script text
Output: Audio file (.mp3 or .wav)
🧠 STAGE 4: Talking Character Generation (Audio + Face)

Tool Options:

    D-ID API (Paid, Free trial)

    HeyGen (Web only, high quality)

    SadTalker (Free, local or Colab)

Input:

    Voiceover audio file

    Character image (yeti, gorilla, alien, etc. — sourced from AI image tools like Bing Image Creator, Midjourney, Leonardo.ai)

Output: Talking-head video clip (.mp4)
🎞️ STAGE 5: Video Editing & Subtitles

Tool Options:

    CapCut Desktop/Web/Mobile (TikTok-ready, easiest)

    FFmpeg (scripted automation for batch processing)

    Runway (stylized backgrounds, free tier)

    Canva Video (browser-based editor)

Steps:

    Add captions (manual or Whisper AI auto-transcription)

    Add sound effects or background music (Pixabay or YouTube Audio Library)

    Add visual overlays (podcast mic, neon lights, glitch effects)

    Export in vertical 9:16 format, 720p or higher resolution

Output: Final short video file (.mp4, under 60 seconds)
🌐 STAGE 6: Upload + Distribution

Platforms:

    YouTube Shorts

    TikTok

    Instagram Reels

    Facebook Reels

Extras:

    Use scheduling tools like Metricool or Buffer

    Add hashtags and viral captions

    Track views, likes, click-through rates

📁 Project Folder Structure

ai-viral-shorts-backend/
│
├── app/                            # Main application package
│   ├── api/                        # Route definitions
│   │   ├── __init__.py
│   │   ├── dialogue.py             # Endpoint to trigger dialogue generation
│   │   └── health.py               # Healthcheck/test endpoint
│   │
│   ├── core/                       # Core configuration
│   │   ├── __init__.py
│   │   └── config.py               # Environment settings and secrets
│   │
│   ├── services/                   # Business logic for each AI pipeline stage
│   │   ├── __init__.py
│   │   ├── dialogue_gen.py           # Script generation using GPT
│   │
│   ├── models/                     # Pydantic models (request/response schemas)
│   │   ├── __init__.py
│   │   └── dialogue_models.py
│   │
│   ├── utils/                      # Utility functions and helpers
│   │   ├── __init__.py
│   │   └── 
│   │
│   └── main.py                     # FastAPI entry point (creates app instance)
│
├── .env                            # Environment variables (not committed)
├── requirements.txt                # Pip dependency list
├── environment.yml                 # Optional: Conda environment definition
├── README.md                       # Project overview and instructions
└── run.sh                          # Bash script to run the server locally

🔍 Description of Key Folders and Files

    app/api/ — Defines REST API routes (e.g., /generate)

    app/services/ — Each AI task (script generation, TTS, animation, editing, upload) in modular service files

    app/models/ — Pydantic schemas for request/response validation

    app/core/config.py — Centralized config using Pydantic’s BaseSettings

    run.sh — Simple runner script for local development using uvicorn